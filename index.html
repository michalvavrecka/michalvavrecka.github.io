<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Michal Vavreƒçka - Personal Web</title>
    <link rel="icon" href="data:image/svg+xml,
<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'>
  <text y='.9em' font-size='90'>üëÅÔ∏è</text>
</svg>">
    <!-- Google Fonts: Inter for a lightweight, modern look -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@200;300;400;600&display=swap" rel="stylesheet">
    
    <style>
        /* --- CSS Variables --- */
        :root {
            --bg-body: #050505;       /* Pure Black/Dark */
            --bg-nav: rgba(5, 5, 5, 0.95);
            --bg-card: #111111;
            --bg-card-hover: #1a1a1a;
            --accent-orange: #ff7f00;
            --text-main: #e0e0e0;
            --text-muted: #888888;
            --border-color: #222222;
        }

        /* --- Global Styles --- */
        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', sans-serif;
            font-weight: 300; /* Lightweight font */
            background-color: var(--bg-body);
            color: var(--text-main);
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        a {
            color: var(--text-main);
            text-decoration: none;
            transition: color 0.2s ease;
        }

        a:hover {
            color: var(--accent-orange);
        }

        /* --- Typography --- */
        h1, h2, h3 {
            font-weight: 400;
            letter-spacing: -0.5px;
            margin-top: 0;
        }

        h1 {
            font-size: 4rem;
            margin-bottom: 0.5rem;
            font-weight: 200;
        }

        h2 {
            font-size: 1.8rem;
            color: var(--accent-orange);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 15px;
            margin-bottom: 30px;
            margin-top: 80px; /* Spacing for sticky nav */
        }

        h3 {
            font-size: 1.2rem;
            color: var(--text-main);
            font-weight: 600;
            margin-bottom: 5px;
        }

        strong {
            font-weight: 600;
            color: #fff;
        }

        /* --- Navigation Bar --- */
        nav {
            position: sticky;
            top: 0;
            width: 100%;
            background-color: var(--bg-nav);
            border-bottom: 1px solid var(--border-color);
            z-index: 1000;
            padding: 15px 0;
            backdrop-filter: blur(10px);
        }

        .nav-container {
            max-width: 1100px;
            margin: 0 auto;
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 25px;
            padding: 0 20px;
        }

        .nav-link {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-muted);
            font-weight: 400;
        }

        .nav-link:hover {
            color: var(--accent-orange);
        }

        /* --- Layout Container --- */
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px 100px 20px;
        }

        /* --- Header / Hero --- */
        .hero {
            display: flex;
            align-items: center;
            gap: 50px;
            padding: 60px 0;
            border-bottom: 1px solid var(--border-color);
        }

        .hero-text {
            text-align: center;
            flex: 1;
        }

        .profile-img {
            width: 220px;
            height: 220px;
            border-radius: 50%;
            overflow: hidden;
            border: 5px solid var(--accent-orange);
            flex-shrink: 0;
        }

        .profile-img img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .hero-text .subtitle {
            color: var(--accent-orange);
            font-size: 1.1rem;
            margin-bottom: 20px;
            display: block;
            text-align: center;
        }

        .contact-info {
            color: var(--text-muted);
            font-size: 0.9rem;
            margin-top: 15px;
            line-height: 1.8;
        }

        .contact-info a {
            color: var(--text-muted);
            border-bottom: 1px solid var(--border-color);
        }

        .contact-info a:hover {
            color: var(--accent-orange);
            border-color: var(--accent-orange);
        }

        /* --- Profile Bio --- */
        .bio {
            font-size: 1.1rem;
            color: #ccc;
            line-height: 1.8;
            max-width: 900px;
        }

        /* --- Timeline (Split Layout) --- */
        .split-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 60px;
        }

        .timeline-entry {
            margin-bottom: 30px;
            padding-left: 20px;
            border-left: 1px solid var(--accent-orange);
        }

        .date {
            color: var(--accent-orange);
            font-size: 0.85rem;
            font-weight: 600;
            display: block;
            margin-bottom: 4px;
        }

        .role {
            font-weight: 600;
            display: block;
            color: #fff;
        }

        .institution {
            color: var(--text-muted);
            font-style: italic;
        }

        .description {
            font-size: 0.9rem;
            margin-top: 5px;
            color: #aaa;
        }

        /* --- Cards (Labs, Projects, Software, Courses) --- */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
            gap: 30px;
        }

        .card {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 20px; /* UPDATED: More round */
            overflow: hidden;
            transition: transform 0.2s, background 0.2s;
            display: flex;
            flex-direction: column;
        }

        .card:hover {
            transform: translateY(-5px);
            background: var(--bg-card-hover);
            border-color: #333;
        }

        /* Container for Videos */
        .video-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 */
            height: 0;
            background: #000;
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: none;
        }

        /* Container for Images (Courses) - Same 16:9 ratio */
        .image-container {
            position: relative;
            padding-bottom: 56.25%; /* 16:9 Aspect Ratio */
            height: 0;
            background: #222; /* Placeholder background */
            overflow: hidden;
        }

        .image-container img {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover; /* Ensures image fills the frame cleanly */
        }

        .card-body {
            padding: 20px;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .card-meta {
            font-size: 0.75rem;
            color: var(--accent-orange);
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 5px;
            display: block;
        }

        .card p {
            font-size: 0.9rem;
            color: #aaa;
            margin-top: 10px;
            margin-bottom: 0;
            text-align: justify;
        }

        .course-link {
            margin-top: auto;
            display: inline-block;
            color: var(--accent-orange);
            font-size: 0.85rem;
            font-weight: 600;
            padding-top: 15px;
        }

        .course-link:hover {
            text-decoration: underline;
        }

        /* --- Publications List --- */
        .pub-section-title {
            color: #fff;
            font-size: 1.4rem;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-left: 10px;
            border-left: 4px solid var(--accent-orange);
        }

        .pub-list {
            list-style: none;
            padding: 0;
        }

        .pub-item {
            display: flex;
            gap: 20px;
            margin-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 20px;
        }

        .pub-year {
            font-weight: 600;
            color: var(--accent-orange);
            font-size: 0.9rem;
            min-width: 50px;
        }

        .pub-content {
            font-size: 0.95rem;
            color: #ccc;
        }

        .pub-link {
            color: var(--text-main);
            text-decoration: none;
            border-bottom: 1px dotted var(--text-muted);
            transition: color 0.2s;
        }

        .pub-link:hover {
            color: var(--accent-orange);
            border-bottom-style: solid;
        }

        .pub-journal {
            font-style: italic;
            color: var(--text-muted);
        }

        /* --- Footer --- */
        footer {
            text-align: center;
            padding: 50px 0;
            color: var(--text-muted);
            font-size: 0.8rem;
            border-top: 1px solid var(--border-color);
            margin-top: 60px;
        }

        /* --- Responsive --- */
        @media (max-width: 768px) {
            .hero {
                flex-direction: column;
                text-align: center;
            }
            .split-grid {
                grid-template-columns: 1fr;
            }
            .pub-item {
                flex-direction: column;
                gap: 5px;
            }
            nav {
                overflow-x: auto;
                white-space: nowrap;
                justify-content: flex-start;
            }
        }
    </style>
</head>
<body>

    <!-- Navigation -->
    <nav>
        <div class="nav-container">
            <a href="#profile" class="nav-link">Profile</a>
            <a href="#experience" class="nav-link">Experience</a>
            <a href="#labs" class="nav-link">Labs</a>
            <a href="#projects" class="nav-link">Projects</a>
            <a href="#software" class="nav-link">Software</a>
            <a href="#courses" class="nav-link">Courses</a>
            <a href="#publications" class="nav-link">Publications</a>
        </div>
    </nav>

    <div class="container">
        
        <!-- Header -->
        <header class="hero" id="profile">
            <div class="profile-img">
                <img src="./images/cv_foto.jpg" alt="Michal Vavreƒçka">
            </div>
            <div class="hero-text">
                <h1>Michal Vavreƒçka</h1>
                <span class="subtitle">Developmental Robotics Researcher</span>
                <div class="contact-info">
                    Czech Institute of Informatics, Robotics and Cybernetics (CIIRC)<br>
                    CTU in Prague, Jugosl√°vsk√Ωch partyz√°n≈Ø 1580/3, 160 00 Prague 6<br><br>
                    <a href="mailto:michal.vavrecka@cvut.cz">michal.vavrecka@cvut.cz</a> |<a href="https://wa.me/420608661977">+420 608 661 977</a> <br>
                    <a href="https://michalvavrecka.github.io">Web</a> | 
                    <a href="https://github.com/michalvavrecka">GitHub</a> | <a href="https://www.linkedin.com/in/michal-vavrecka-3072529/">LinkedIn</a>| <a href="https://scholar.google.com/citations?user=AhgkbbIAAAAJ&hl=cs">Scholar</a> | <a href="./articles/MichalVavrecka_CV2025_ENG.pdf">CV</a>
                </div>
            </div>
        </header>

        <section>
            <h2>Personal Profile</h2>
            <p class="bio">
                I am a cognitive scientist specializing in developmental robotics, artificial intelligence, and cognitive modeling. Since the beginning of my research in 2005, I have been dedicated to the principles of Artificial General Intelligence (AGI). During my psychology studies, I was already creating my own cognitive architectures. I continue this work by testing cognitive architectures in humanoid robots, which I train according to the principles of developmental robotics using reinforcement learning and intrinsic motivation, utilizing advanced robotic simulators that enable the automated creation of long task sequences (lifelong learning). In my research, I connect findings from psychology and neuroscience with technical applications in robotics.
            </p>
        </section>

        <!-- Experience & Education -->
        <section id="experience">
            <div class="split-grid">
                <div>
                    <h2>Experience</h2>
                    <div class="timeline-entry">
                        <span class="date">04/2017 ‚Äì present</span>
                        <span class="role">Researcher</span>
                        <span class="institution">Czech Institute of Informatics, Robotics and Cybernetics, CTU in Prague</span>
                        <div class="description">Development of algorithms for humanoid robots and research in human-robot interaction.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">10/2022 ‚Äì 10/2025</span>
                        <span class="role">Researcher</span>
                        <span class="institution">Faculty of Mathematics, Physics and Informatics, CU in Bratislava, </span>
                        <div class="description">Development of algorithms for humanoid robots (Faculty of Mathematics, Physics and Informatics).</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">01/2019 ‚Äì 12/2021</span>
                        <span class="role">Researcher</span>
                        <span class="institution">Institute of Physiology, Academy of Sciences</span>
                        <div class="description">Advanced EEG signal processing and research in spatial cognition.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">04/2016 ‚Äì 12/2018</span>
                        <span class="role">Researcher</span>
                        <span class="institution">Institute of Computer Science, Academy of Sciences</span>
                        <div class="description">Advanced EEG signal processing and research in neuroscience.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">07/2014 ‚Äì 12/2015</span>
                        <span class="role">Researcher</span>
                        <span class="institution">EEG department, National Institute of Mental Health, Klecany</span>
                        <div class="description">Development of algorithms for advanced EEG signal processing and research in sychology.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">12/2012 ‚Äì 06/2015</span>
                        <span class="role">Postdoc</span>
                        <span class="institution">Psychology Department, University of South Bohemia in ƒåesk√© Budƒõjovice</span>
                        <div class="description">Neuropsychological research in the field of emotions and EEG signal processing.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">04/2008 ‚Äì 12/2016</span>
                        <span class="role">Assistant Professor</span>
                        <span class="institution">Faculty of Electrical Engineering, CTU in Prague</span>
                        <div class="description">Research in spatial cognition, development of algorithms for EEG processing.</div>
                    </div>
                </div>

                <div>
                    <h2>Education</h2>
                    <div class="timeline-entry">
                        <span class="date">2017</span>
                        <span class="role">Research stay</span>
                        <span class="institution">Tokyo University of Agriculture and Technology, Kondo Lab</span>
                        <div class="description">Japan</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">2014</span>
                        <span class="role">Research stay</span>
                        <span class="institution">Technical University of Berlin, Dept. of Biopsychology</span>
                        <div class="description">Germany</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">2009</span>
                        <span class="role">Research stay</span>
                        <span class="institution">Comenius University Bratislava, Faculty of Mathematics, Physics and Informatics</span>
                        <div class="description">Slovakia</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">2005 ‚Äì 2008</span>
                        <span class="role">Ph.D., General Psychology</span>
                        <span class="institution">Masaryk University, Brno</span>
                        <div class="description">Dissertation: Application of cognitive semantics in the model of spatial relations representation.</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">2006</span>
                        <span class="role">Research stay</span>
                        <span class="institution">Comenius University Bratislava, Faculty of Mathematics, Physics and Informatics</span>
                        <div class="description">Slovakia</div>
                    </div>
                    <div class="timeline-entry">
                        <span class="date">1999 ‚Äì 2005</span>
                        <span class="role">Mgr. (M.A.), Psychology</span>
                        <span class="institution">Masaryk University, Brno</span>
                        <div class="description">Master‚Äôs Thesis: Methods of Human Intelligence Simulation.</div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Founded Labs -->
        <section id="labs">
            <h2>Founded Labs</h2>
            <div class="card-grid">
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/Incognite.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2014</span>
                        <h3><a href="https://incognite-lab.github.io/" target="_blank">Incognite Lab</a></h3>
                        <p>We focus on the development of embodied cognitive architectures that are based on an epigenetic robotics principles. We adopt both industrial and humanoid robots to make them learn how to understand the outside world. We develop software for training humanoid robots, such as myGym and PRAG.</p>
                    </div>
                </div>
                <div class="card">
                     <div class="image-container">
                        <img src="./images/eeglabjcu.jpg" alt="EEGlab">
                    </div>
                    <div class="card-body">
                        <span class="card-meta">2012</span>
                        <h3><a href="https://old.pf.jcu.cz/structure/departments/kpe/neurolab/index.html" target="_blank">Neuropsychological Lab at USB</a></h3>
                        <p>Open access laboratory equipped with 64-channel EEG and eye-tracker. The laboratory serves for administering experiments in cognitive and affective neuroscience.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/biodat.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2009</span>
                        <h3><a href="https://www.ciirc.cvut.cz/teams-labs/cogsys/" target="_blank">EEG Lab at FEE CTU</a></h3>
                        <p>Research laboratory dedicated to investigating spatial cognition with a particular emphasis on reference frames and neural representations of space. The lab is equipped for EEG-based experiments that study how the human brain processes, integrates, and switches between different spatial reference frames during perception, movement, and interaction with the environment.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Projects -->
        <section id="projects">
            <h2>Projects</h2>
            <div class="card-grid">
                <!-- Existing Projects -->
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/ichores_final.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2023-2025</span>
                        <h3><a href="http://ichores.ciirc.cvut.cz/" target="_blank">iChores</a></h3>
                        <p>This European project focuses on investigating the methods that enable a collaborative robot to extract task‚Äërelevant information from the gaze and gestures of a human partner, and how gaze, gesture, and speech information can be combined for a more natural and intuitive interface.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/nico_mygym.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2022-2025</span>
                        <h3><a href="https://terais.eu/" target="_blank">TERAIS</a></h3>
                        <p>TERAIS (Towards Excellent Robotics and Artificial Intelligence at a Slovak University) is an international project aimed at establishing the Department of Applied Informatics of Comenius University Bratislava. Funded by the Horizon Europe programme under the Twinning call, TERAIS brings together experts from the University of Hamburg in Germany and the Italian Institute of Technology in Genoa to collaborate with scientists at DAI</p>
                    </div>
                </div>
                <div class="card">
                     <div class="image-container">
                        <img src="./images/mirracle.png" alt="mirracle">
                    </div>
                    <div class="card-body">
                        <span class="card-meta">2021-2024</span>
                        <h3><a href="http://imitrob.ciirc.cvut.cz/projects/mirracle/index.html" target="_blank">MIRRACLE</a></h3>
                        <p>The goal of the project is to design a mapping algorithm that will enable us to create such a multimodal representation of actions that incorporates prior knowledge about the uncertainty of different domains, and show how this can be used to ease teaching robotic actions and generalize them to new embodiments (e.g. different robots, grippers, etc.) and environment.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/HUMR.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2019-2021</span>
                        <h3><a href="https://www.fss.muni.cz/vyzkum/resene-projekty/43869" target="_blank">HUMR</a></h3>
                        <p>The project focuses on researching the social and technological aspects of the relationship between an elderly human user and the humanoid robot Pepper (human - robot interaction; HRI). The possibilities of using the robot to activate seniors in leisure centers for seniors are being explored.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/tradr.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2019-2021</span>
                        <h3><a href="https://www.tradr-project.eu/" target="_blank">Tradr</a></h3>
                        <p>TRADR is an integrated european research project in the area of robot-assisted disaster response. Using a proven-in-practice user-centric design methodology, TRADR develops novel science and technology for human-robot teams to assist in urban search and rescue disaster response efforts, which stretch over multiple sorties in missions that may take several days or weeks.</p>
                    </div>
                </div>
                <div class="card">
                     <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/homunculus.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2014-2017</span>
                        <h3><a href="https://cyber.felk.cvut.cz/news/new-gacr-expro-project/" target="_blank">Robotic Homunculus</a></h3>
                        <p>Inspired by the biological somatosensory cortex, this project uses the iCub humanoid robot equipped with artificial skin. It investigates how the robot can autonomously learn a representation of its own body surface (a "robotic homunculus") through tactile stimulation and self-organizing maps.</p>
                    </div>
                </div>
                <div class="card">
                   <div class="image-container">
                        <img src="./images/vireas.png" alt="VIREAS">
                    </div>
                    <div class="card-body">
                        <span class="card-meta">2010-2014</span>
                        <h3><a href="https://vireas.cz/" target="_blank">VIREAS</a></h3>
                        <p>VIREAS software ‚Äì Virtual Experience Kit ‚Äì was developed based on a series of tests and studies among residents of a nursing home. The kit includes three virtual experiences: ‚ÄúWalk in the Forest‚Äù, ‚ÄúWalk in the City Center‚Äù and ‚ÄúTravel‚Äù. The interactive design combines computer graphics and 360¬∞ photography. The kit is designed to positively influence the self-expression and self-confidence of seniors, motivate them and stimulate their curiosity. </p>
                    </div>
                </div>
                <div class="card">
                     <div class="image-container">
                        <img src="./images/vrbone.jpeg" alt="VR Bone Assembly 2.0">
                    </div>
                    <div class="card-body">
                         <span class="card-meta">2015-2017</span>
                        <h3><a href="https://github.com/incognite-lab/" target="_blank">VR Bone Assembly 2.0</a></h3>
                        <p>VR Bone Assembly 2.0 is an immersive educational software designed for medical students that uses virtual reality to teach human skeletal anatomy through interactive, hands-on learning. The application allows users to assemble and explore bones in a realistic 3D environment, improving spatial understanding and anatomical accuracy while reinforcing theoretical knowledge through practical engagement.</p>
                    </div>
                </div>
                
                <div class="card">
                <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="70%"
    >
                    <source src="./videos/clopema.mp4" type="video/mp4">
                 </video>
                </div>
                <div class="card-body">
                        <span class="card-meta">2012-2015</span>
                        <h3><a href="https://robotics.iti.gr/en/clopema-2/" target="_blank">CloPeMa</a></h3>
                        <p>CloPeMa aims to advance the state of the art in the autonomous perception and manipulation of all kinds of fabrics, textiles and garments. Various garments will be presented in a random pile on an arbitrary background and novel ways of manipulating them (sorting, folding, etc.) will be learned on demand in a real-life dynamic environment.</p>
                </div>
                </div>
                <!-- New Projects (From Wix) -->

                <div class="card">
                   <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/ripley.mp4" type="video/mp4">
        Your browser does not support the video tag.
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2012-2016</span>
                        <h3><a href="https://drnikolaosmavridis.com/ripley-project.php" target="_blank">Ripley Robot</a></h3>
                        <p>Extension of Ripley robot cognitive architecture from Nikos Mavridis (MIT). A cognitive robotics project utilizing a 7-DOF manipulator with rich sensory capabilities (vision, audio, touch). The research focuses on grounding natural language semantics in perception and action, enabling collaborative tasks through spoken dialogue and interactive learning.</p>
                    </div>
                    </div>



                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/eegvoxel.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2011-2014</span>
                        <h3><a href="https://scholar.google.com/citations?view_op=view_citation&hl=cs&user=AhgkbbIAAAAJ&citation_for_view=AhgkbbIAAAAJ:_FxGoFyzp5QC" target="_blank">Affective Neuroscience</a></h3>
                        <p>Neuropsychological research focused on classifying human emotional states using EEG analysis. By presenting affective stimuli (such as IAPS images), the project aims to identify specific neural correlates and feature patterns associated with different emotional responses.</p>
                    </div>
                </div>
                <div class="card">
                   <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/neurofeedback.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2010-2014</span>
                        <h3><a href="https://github.com/incognite-lab/" target="_blank">Neurofeedback Software</a></h3>
                        <p>In this projekct we developed applications for EEG-based neurofeedback that strongly emphasize gamification to enhance user engagement and learning outcomes. By integrating game-like mechanics such as real-time feedback, goals, rewards, and adaptive challenges, the software transforms neural self-regulation tasks into interactive experiences while maintaining rigorous experimental and clinical relevance.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/eegnumberlines.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <span class="card-meta">2008-2014</span>
                        <h3><a href="https://michalvavrecka.github.io/articles/vavrecka_visneuro2012.pdf" target="_blank">Spatial Cognition</a></h3>
                        <p>An EEG-based study investigating how the human brain processes spatial information during navigation. The research differentiates between allocentric (world-centered) and egocentric (body-centered) reference frames using data from virtual environment tunnel tasks.</p>
                    </div>
                </div>

                <div class="card">
                    <div class="image-container">
                        <img src="./images/vrstay.jpg" alt="VRstay">
                    </div>
                    <div class="card-body">
                        <span class="card-meta">2005-2007</span>
                        <h3><a href="https://michalvavrecka.github.io/articles/vavrecka_kog2006.pdf" target="_blank">One week continuos stay in VR</a></h3>
                        <p>The presented paper contains results of pilot study in the area of human computer interaction.
The proposed design is based on a subject's stay of several days in purely symbolic (text) visual
environment and subsequent analysis of one¬¥s change in production and perception of visual
mental images.</p>
                    </div>
                </div>
                <div class="card">
                <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/VQA.mp4" type="video/mp4">
                 </video>
                </div> 
                    <div class="card-body">
                        <span class="card-meta">2003-present</span>
                        <h3><a href="https://link.springer.com/article/10.1007/s12559-013-9212-5" target="_blank">Symbol Grounding</a></h3>
                        <p>This fundamental research addresses the symbol grounding problem: how abstract symbols (like words) gain meaning through connection to physical world experiences. The project develops computational models that link linguistic inputs with sensorimotor data from robotic agents.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Software -->
        <section id="software">
            <h2>Software</h2>
            <div class="card-grid">
                <div class="card">
                   <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/PRAG.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <h3><a href="https://github.com/incognite-lab/prag" target="_blank">PRAG</a></h3>
                        <p>Our generator takes as input user-defined sets of atomic actions, objects, and spatial predicates and outputs solvable tasks of a given length for the selected robotic environment. The generator produces solvable tasks by constraining all possible (nonsolvable) combinations by symbolic and physical validation.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/mygym.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <h3><a href="https://github.com/incognite-lab/myGym" target="_blank">myGym</a></h3>
                        <p>myGym enables fast prototyping of RL in the area of robotic manipulation and navigation.You can train different robots, in several environments on various tasks. There is automatic evaluation and benchmark tool. From version 2.1 there is support for multi-step tasks, multi-reward training and multi-network architectures.</p>
                    </div>
                </div>
                                <div class="card">
                   <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/Tiago.mp4" type="video/mp4">
                 </video>
                </div>
                <div class="card-body">
                        <h3><a href="https://github.com/orgs/ichores-research/repositories" target="_blank">iChores</a></h3>
                        <p>We have developed and demonstrated a system that en-
ables the TIAGo++ robot to understand and execute complex,
sequential commands that combine natural language and
multiple pointing gestures. Our approach of decomposing
commands and using a probabilistic framework for reasoning
has proven to be highly effective in the planning stage.</p>
                    </div>
                </div>
                <div class="card">
                <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/nico_handeye.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <h3><a href="https://github.com/Robotics-DAI-FMFI-UK/NicoIK" target="_blank">NicoIK</a></h3>
                        <p>Inverse kinematics and automatic grasping for the Nico humanoid robot.</p>
                    </div>
                </div>
                <div class="card">
                   <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/Pepper.mp4" type="video/mp4">
                 </video>
                </div>
                <div class="card-body">
                        <h3><a href="https://github.com/incognite-lab/Pepper-Controller" target="_blank">Pepper Controler</a></h3>
                        <p>Controlling the Pepper humanoid robot using Python.</p>
                    </div>
                </div>
                <div class="card">
                    <div class="video-container">
                    <video
                    controls
                    playsinline
                    preload="metadata"
                    width="100%"
    >
                    <source src="./videos/Bioloid.mp4" type="video/mp4">
                 </video>
                </div>
                    <div class="card-body">
                        <h3><a href="https://github.com/incognite-lab/Bioloid-Toolbox" target="_blank">Bioloid Toolbox</a></h3>
                        <p>A specialized software control library (Matlab/Python) designed for the Robotis Bioloid robot kits.</p>
                    </div>
                </div>

                
            </div>
        </section>

        <!-- Courses -->
        <section id="courses">
            <h2>Courses</h2>
            <div class="card-grid">
                
                <div class="card">
                    <div class="image-container">
                        <img src="./images/cognitivesystems.jpg" alt="Cognitive Systems">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://cw.fel.cvut.cz/b241/courses/a6m33ksy/start"_blank">Cognitive Systems</a></h3>
                        <p>Focuses on systems that can perceive, reason, learn, and interact with the environment, often drawing inspiration from biological cognitive systems.</p>
                    </div>
                </div>
               
                <div class="card">
                    <div class="image-container">
                        <img src="./images/cogsci.jpg" alt="Cognitive Science">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://is.muni.cz/predmet/fss/podzim2009/PSY481" target="_blank">Cognitive Science</a></h3>
                        <p>An introduction to the interdisciplinary study of mind and intelligence, embracing philosophy, psychology, artificial intelligence, neuroscience, linguistics, and anthropology.</p>
                    </div>
                </div>

                <div class="card">
                    <div class="image-container">
                        <img src="./images/metacognition.jpg" alt="Metacognition">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://is.muni.cz/el/1423/jaro2013/PSY258/PSY258_2013.pdf?lang=cs" target="_blank">Metacognition</a></h3>
                        <p>Exploration of "thinking about thinking", including knowledge about when and how to use particular strategies for learning or problem-solving.</p>
                    </div>
                </div>

                <div class="card">
                    <div class="image-container">
                        <img src="./images/neuropsychology.jpg" alt="Neuropsychology">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://www.pf.jcu.cz/cz/fakulta/katedry/katedra-psychologie" target="_blank">Neuropsychology</a></h3>
                        <p>Study of the structure and function of the brain as they relate to specific psychological processes and behaviors.</p>
                    </div>
                </div>

                <div class="card">
                    <div class="image-container">
                        <img src="./images/neuroinformatics.jpg" alt="Neuroinformatics">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://intranet.fel.cvut.cz/en/education/bk/predmety/54/67/p5467506.html" target="_blank">Neuroinformatics</a></h3>
                        <p>Application of computational models and analytical tools to neuroscience data, bridging the gap between neuroscience and information science.</p>
                    </div>
                </div>

                <div class="card">
                    <div class="image-container">
                        <img src="./images/psychometrics.jpg" alt="Psychometrics">
                    </div>
                    <div class="card-body">
                        <h3><a href="https://www.pf.jcu.cz/cz/fakulta/katedry/katedra-psychologie" target="_blank">Psychometrics</a></h3>
                        <p>The field of study concerned with the theory and technique of psychological measurement, including the measurement of knowledge, abilities, attitudes, and personality traits.</p>
                    </div>
                </div>

            </div>
        </section>

        <!-- Publications -->
        <section id="publications">
            <h2>List of Publications</h2>

            <div class="pub-section-title">Journal Articles</div>
            <ul class="pub-list">
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Will%20multimodal%20large%20language%20models%20ever%20achieve%20deep%20understanding%20of%20the%20world%3F%20Vavrecka" target="_blank">
                            Farka≈°, I., <strong>Vavreƒçka, M.</strong>, & Wermter, S. Will multimodal large language models ever achieve deep understanding of the world?
                        </a> <span class="pub-journal">Frontiers in Systems Neuroscience, 19, 1683133.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2018</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Mapping%20language%20to%20vision%20in%20a%20real-world%20robotic%20scenario%20Vavrecka" target="_blank">
                            ≈†tƒõp√°nov√°, K., Klein, F. B., Cangelosi, A., & <strong>Vavreƒçka, M.</strong> Mapping language to vision in a real-world robotic scenario.
                        </a> <span class="pub-journal">IEEE Transactions on Cognitive and Developmental Systems, 10(3), 784-794.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2017</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Robotic%20homunculus%3A%20Learning%20of%20artificial%20skin%20representation%20in%20a%20humanoid%20robot%20Vavrecka" target="_blank">
                            Hoffmann, M., Straka, Z., Farkas, I., <strong>Vavreƒçka, M.</strong> & Metta, G. Robotic homunculus: Learning of artificial skin representation in a humanoid robot motivated by primary somatosensory cortex.
                        </a> <span class="pub-journal">IEEE Transactions on Cognitive and Developmental Systems.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2017</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=%C3%9Avod%20do%20aproxim%C3%A1ln%C3%ADho%20numerick%C3%A9ho%20syst%C3%A9mu%20Vavrecka" target="_blank">
                            Plassov√°, M., Stuchl√≠kov√°, I., <strong>Vavreƒçka, M.</strong> √övod do aproxim√°ln√≠ho numerick√©ho syst√©mu.
                        </a> <span class="pub-journal">Pedagogika, 67 (2).</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2016</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Estimating%20number%20of%20components%20in%20Gaussian%20mixture%20model%20Vavrecka" target="_blank">
                            ≈†tƒõp√°nov√° K., <strong>Vavreƒçka M.</strong> Estimating number of components in Gaussian mixture model using combination of greedy and merging algorithm.
                        </a> <span class="pub-journal">Pattern Analysis and Applications.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2016</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Kognice%20a%20hemodynamika%20po%20karotick%C3%A9%20endarterektomii%20Vavrecka" target="_blank">
                            Fiedler, J., Mrh√°lek, T., <strong>Vavreƒçka, M.</strong>, et al. Kognice a hemodynamika po karotick√© endarterektomii pro asymptomatickou sten√≥zu.
                        </a> <span class="pub-journal">ƒåesk√° a slovensk√° neurologie a neurochirurgie, 79/112(2), 201‚Äì206.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2015</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Cognitive%20Outcome%20and%20Hemodynamic%20Changes%203%20months%20after%20Carotid%20Endarterectomy%20Vavrecka" target="_blank">
                            Fiedler, J., ... <strong>Vavreƒçka, M.</strong>, et al. Cognitive Outcome and Hemodynamic Changes 3 months after Carotid Endarterectomy.
                        </a> <span class="pub-journal">Journal of Neurosurgery, 123(2).</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2014</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=A%20multimodal%20connectionist%20architecture%20for%20unsupervised%20grounding%20of%20spatial%20language%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, & Farka≈°, I. A multimodal connectionist architecture for unsupervised grounding of spatial language.
                        </a> <span class="pub-journal">Cognitive Computation, 6(1), 101-112.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2012</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Frames%20of%20reference%20and%20their%20neural%20correlates%20within%20navigation%20in%20a%203D%20environment%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Gerla, V., Lhotsk√°, L., & Brunovsk√Ω, M. Frames of reference and their neural correlates within navigation in a 3D environment.
                        </a> <span class="pub-journal">Visual Neuroscience, 29(03), 183-191.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2012</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Odli%C5%A1nosti%20u%C5%BE%C3%ADv%C3%A1n%C3%AD%20referen%C4%8Dn%C3%ADch%20r%C3%A1mc%C5%AF%20b%C4%9Bhem%20orientace%20v%203D%20prostoru%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, & Lhotsk√°, L. Odli≈°nosti u≈æ√≠v√°n√≠ referenƒçn√≠ch r√°mc≈Ø bƒõhem orientace v 3D prostoru.
                        </a> <span class="pub-journal">Linguistica ONLINE, 14, 95-100.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2012</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=The%20alteration%20of%20reference%20frames%20in%20a%20vertical%20navigation%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, & Lhotsk√°, L. The alteration of reference frames in a vertical navigation.
                        </a> <span class="pub-journal">Cognitive Processing, Springer Berlin.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2012</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=The%20inter-individual%20differences%20and%20the%20test%E2%80%93retest%20reliability%20of%20a%20EEG%20signal%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Rosset, B., & Lhotsk√°, L. The inter-individual differences and the test‚Äìretest reliability of a EEG signal.
                        </a> <span class="pub-journal">Clinical Neurophysiology, 123(3).</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2011</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Event%20related%20spectral%20perturbations%20within%203D%20navigation%20task%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, & Lhotsk√°, L. Event related spectral perturbations within 3D navigation task.
                        </a> <span class="pub-journal">Activitas Nervosa Superior Rediviva, 53.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2009</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=The%20neural%20correlates%20of%20spatial%20reference%20frames%20processing%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong> The neural correlates of spatial reference frames processing.
                        </a> <span class="pub-journal">Cognitive Processing, 10:2, 342-345.</span>
                    </div>
                </li>
            </ul>

            <div class="pub-section-title">Conference Papers</div>
            <ul class="pub-list">
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Are%20Multimodal%20Signals%20Synchronous%3F%20Vavrecka" target="_blank">
                            Zamrazilova, K., <strong>Vavrecka, M.</strong>, et al. ‚ÄúAre Multimodal Signals Synchronous?‚Äù: Temporal Relation of Declarative Gestures and Language Instructions in Human Robot Interaction.
                        </a> <span class="pub-journal">2025 IEEE International Conference on Development and Learning (ICDL), 1-6.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=PRAG%3A%20Procedural%20Action%20Sequence%20Symbolic%20Generator%20Vavrecka" target="_blank">
                            ≈†koviera, R., <strong>Vavreƒçka, M.</strong>, et al. PRAG: Procedural Action Sequence Symbolic Generator as a Mechanism for Autonomous Learning.
                        </a> <span class="pub-journal">2025 IEEE International Conference on Development and Learning (ICDL), 1-8.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Robotic%20Calibration%20Based%20on%20Haptic%20Feedback%20Improves%20Sim-to-Real%20Transfer%20Vavrecka" target="_blank">
                            Gavura, J., <strong>Vavrecka, M.</strong>, Farka≈°, I., & G√§de, C. Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer.
                        </a> <span class="pub-journal">International Conference on Artificial Neural Networks (ICANN), 136-148.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Examining%20the%20legibility%20of%20humanoid%20robot%20arm%20movements%20Vavrecka" target="_blank">
                            L√∫ƒçny, A., ... & <strong>Vavrecka, M.</strong> Examining the legibility of humanoid robot arm movements in a pointing task.
                        </a> <span class="pub-journal">arXiv preprint arXiv:2508.05104.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2025</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=PRAG%3A%20Procedural%20Action%20Generator%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Skoviera, R., Sejnova, G., & Stepanova, K. PRAG: Procedural Action Generator.
                        </a> <span class="pub-journal">arXiv preprint arXiv:2507.09167.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2024</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Bridging%20language%2C%20vision%20and%20action%3A%20Multimodal%20VAEs%20Vavrecka" target="_blank">
                            Sejnova, G., <strong>Vavrecka, M.</strong>, & Stepanova, K. Bridging language, vision and action: Multimodal VAEs in robotic manipulation tasks.
                        </a> <span class="pub-journal">2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2024</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Modular%20Reinforcement%20Learning%20In%20Long-Horizon%20Manipulation%20Tasks%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Kriz, J., Sokovnin, N., & Sejnova, G. Modular Reinforcement Learning In Long-Horizon Manipulation Tasks.
                        </a> <span class="pub-journal">International Conference on Artificial Neural Networks (ICANN), 299-312.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2024</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Action%20Recognition%20System%20Integrating%20Motion%20and%20Object%20Detection%20Vavrecka" target="_blank">
                            Ostapenko, A., & <strong>Vavrecka, M.</strong> Action Recognition System Integrating Motion and Object Detection.
                        </a> <span class="pub-journal">International Conference on Artificial Neural Networks (ICANN), 259-269.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2024</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Adaptive%20compression%20of%20the%20latent%20space%20in%20variational%20autoencoders%20Vavrecka" target="_blank">
                            Sejnova, G., <strong>Vavrecka, M.</strong>, & Stepanova, K. Adaptive compression of the latent space in variational autoencoders.
                        </a> <span class="pub-journal">International Conference on Artificial Neural Networks (ICANN), 89-101.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2023</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Native%20Czech%20speakers%20consider%20English-speaking%20robots%20more%20intelligent%20Vavrecka" target="_blank">
                            Sienkiewicz, B., Sejnova, G., Gajewski, P., <strong>Vavrecka, M.</strong>, & Indurkhya, B. Native Czech speakers consider English-speaking robots more intelligent.
                        </a> <span class="pub-journal">Proceedings of the 11th International Conference on Human-Agent Interaction (HAI), 362-364.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2023</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=How%20language%20of%20interaction%20affects%20the%20user%20perception%20of%20a%20robot%20Vavrecka" target="_blank">
                            Sienkiewicz, B., ... <strong>Vavrecka, M.</strong>, & Indurkhya, B. How language of interaction affects the user perception of a robot.
                        </a> <span class="pub-journal">International Conference on Social Robotics (ICSR), 308-321.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2022</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Personified%20Robotic%20Chatbot%20Based%20On%20Compositional%20Dialogues%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Sejnova, G., & Schimperk, P. Personified Robotic Chatbot Based On Compositional Dialogues.
                        </a> <span class="pub-journal">International Conference on Interactive Media, Smart Systems and Emerging Technologies (IMET), 1-2.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2021</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=myGym%3A%20Modular%20toolkit%20for%20visuo-motor%20robotic%20tasks%20Vavrecka" target="_blank">
                            <strong>Vavrecka, M.</strong>, Sokovnin, N., Mejdrechova, M., & Sejnova, G. myGym: Modular toolkit for visuo-motor robotic tasks.
                        </a> <span class="pub-journal">2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI), 279-283.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2021</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Reward%20Redistribution%20for%20Reinforcement%20Learning%20Vavrecka" target="_blank">
                            Sejnova, G., Mejdrechova, M., ... & <strong>Vavrecka, M.</strong> Reward Redistribution for Reinforcement Learning of Dynamic Nonprehensile Manipulation.
                        </a> <span class="pub-journal">2021 7th International Conference on Control, Automation and Robotics (ICCAR), 326-331.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2020</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=A%20Learning%20Based%20Approach%20for%20Planning%20with%20Safe%20Actions%20Vavrecka" target="_blank">
                            Niyogi, R., Sharma, S., <strong>Vavrecka, M.</strong>, & Milani, A. A Learning Based Approach for Planning with Safe Actions.
                        </a> <span class="pub-journal">Computational Science and Its Applications ‚Äì ICCSA 2020, 93-105.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2019</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Exploring%20logical%20consistency%20and%20viewport%20sensitivity%20Vavrecka" target="_blank">
                            Sejnova, G., <strong>Vavrecka, M.</strong>, Tesar, M., & Skoviera, R. Exploring logical consistency and viewport sensitivity in compositional VQA models.
                        </a> <span class="pub-journal">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2108-2113.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2018</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Compositional%20models%20for%20VQA%3A%20Can%20neural%20module%20networks%20really%20count%3F%20Vavrecka" target="_blank">
                            Sejnova, G., Tesar, M., & <strong>Vavrecka, M.</strong> Compositional models for VQA: Can neural module networks really count?
                        </a> <span class="pub-journal">Procedia computer science, 145, 481-487.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2016</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Approximate%20number%20system%20in%20children%20Vavrecka" target="_blank">
                            Plassov√°, M., Tesa≈ô, M., <strong>Vavreƒçka, M.</strong>, & Valuchov√°, K. Approximate number system in children.
                        </a> <span class="pub-journal">Proceedings of the 6th Biannual CER Comparative European Research Conference, 182-187.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2013</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Vliv%20kontextu%20na%20orientaci%20v%20prostoru%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, Ku≈æ√≠lek, J., & Lhotsk√°, L. Vliv kontextu na orientaci v prostoru.
                        </a> <span class="pub-journal">Kogn√≠cia a umely ≈æivot XIII, 279-285.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2013</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=EEG%20Feature%20Selection%20Based%20on%20Time%20Series%20Classification%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, & Lhotsk√°, L. EEG Feature Selection Based on Time Series Classification.
                        </a> <span class="pub-journal">Lecture Notes in Computer Science (LNCS), vol. 7988, 520-527.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2012</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Unsupervised%20Visual%20and%20Lexical%20Binding%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, & Farka≈°, I. Unsupervised Visual and Lexical Binding.
                        </a> <span class="pub-journal">Proceedings of International Conference on Cognitive Systems.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2011</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Bio-inspired%20Model%20of%20Spatial%20Cognition%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, Farka≈°, I., & Lhotsk√°, L. Bio-inspired Model of Spatial Cognition.
                        </a> <span class="pub-journal">Lecture Notes in Computer Science 7062 (LNCS), 443-450.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2011</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=EEG%20analysis%20of%20the%20navigation%20strategies%20in%20a%203D%20tunnel%20task%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, & Lhotsk√°, L. EEG analysis of the navigation strategies in a 3D tunnel task.
                        </a> <span class="pub-journal">Lecture Notes in Computer Science 7062 (LNCS), 388-395.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2011</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Visually%20driven%20homonyms%20disambiguation%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Visually driven homonyms disambiguation.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot XI, 293-299.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2011</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Unsupervised%20Grounding%20of%20Spatial%20Relations%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, & Farka≈°, I. Unsupervised Grounding of Spatial Relations.
                        </a> <span class="pub-journal">Proceedings of the Third European Conference on Cognitive Science, Bulgaria.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2010</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Unsupervised%20model%20for%20grounding%20multimodal%20representations%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, & Farka≈°, I. Unsupervised model for grounding multimodal representations.
                        </a> <span class="pub-journal">Third EuCogII Members Conference, Mallorca.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2010</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Classification%20of%20the%20EEG%20feature%20components%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong>, Ku≈æ√≠lek J., & Lhotsk√°, L. Classification of the EEG feature components.
                        </a> <span class="pub-journal">10th International Conference on Information Technology and Applications in Biomedicine.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2010</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Representation%20of%20objects%20in%20space%20based%20on%20two%20visual%20pathways%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Representation of objects in space based on two visual pathways.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot X, 403-409.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2009</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Classification%20of%20the%20emotional%20states%20based%20on%20the%20EEG%20signal%20processing%20Vavrecka" target="_blank">
                            Maca≈°, M., <strong>Vavreƒçka, M.</strong>, Gerla V., & Lhotsk√°, L. Classification of the emotional states based on the EEG signal processing.
                        </a> <span class="pub-journal">9th International Conference on Information Technology and Applications in Biomedicine.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2009</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=The%20EEG%20Correlates%20of%20the%20Allocentric%20and%20the%20Egocentric%20Spatial%20Reference%20Frames%20Processing%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> The EEG Correlates of the Allocentric and the Egocentric Spatial Reference Frames Processing.
                        </a> <span class="pub-journal">World Congress on Medical Physics and Biomedical Engineering, 2295-2299.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2009</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=The%20EEG%20activity%20within%20the%20spatial%20navigation%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> The EEG activity within the spatial navigation.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot IX, 341-350.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2008</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Spatial%20frames%20of%20reference%20processing%20and%20its%20EEG%20correlates%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Spatial frames of reference processing and its EEG correlates.
                        </a> <span class="pub-journal">55. spoleƒçn√Ω sjezd ƒçesk√© a slovensk√© spoleƒçnosti klinick√© neurofyziologie.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2008</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Multimodal%20representations%20for%20symbol%20grounding%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Multimodal representations for symbol grounding.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot VIII.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2007</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Grounding%20of%20spatial%20terms%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Grounding of spatial terms.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot VII, 365-377.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2006</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=Symbol%20grounding%20in%20context%20of%20zero%20semantic%20commitment%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> Symbol grounding in context of zero semantic commitment.
                        </a> <span class="pub-journal">Kognice a umƒõl√Ω ≈æivot VI, 401-411.</span>
                    </div>
                </li>
                <li class="pub-item">
                    <div class="pub-year">2006</div>
                    <div class="pub-content">
                        <a class="pub-link" href="https://scholar.google.com/scholar?q=How%20long-term%20presentation%20of%20text%20stimuli%20affects%20production%20and%20perception%20Vavrecka" target="_blank">
                            <strong>Vavreƒçka, M.</strong> How long-term presentation of text stimuli affects production and perception of visual mental images.
                        </a> <span class="pub-journal">Kognice 2006, 150-158.</span>
                    </div>
                </li>
            </ul>
        </section>

        <footer>
            <p>&copy; 2003-2033 Michal Vavreƒçka</p>
            <div style="margin-top: 10px;">
                <!-- hitwebcounter Code START -->
            <a href="https://www.hitwebcounter.com/" target="_blank">
            <img src="https://hitwebcounter.com/counter/counter.php?page=21464016&style=0007&nbdigits=5&type=page&initCount=7" title="Free Tools" Alt="Free Tools"   border="0" /></a>  
            </div>
        </footer>

    </div>
</body>
</html>
